% pro cited nutno pouzit vhodny .bst (nebo biblatex)
@MISC{alchemy,
  key = "AlchemyAPI",
  author = {{AlchemyAPI, Inc.}},
  title = "AlchemyAPI",
  year = 2013,
  cited = "11. 7. 2013",
  note = "\url{www.alchemyapi.com}"
}

@MISC{opencalais,
  key = "OpenCalais",
  author = {{Thomson Reuters Corporation}},
  title = "OpenCalais",
  year = 2013,
  cited = "18. 9. 2013",
  note = "\url{www.opencalais.com}"
}
@MISC{linkingopendata,
  key = "Linking Open Data",
  author = {{W3C SWEO Community Project}},
  title = "Linking Open Data",
  year = 2013,
  cited = "18. 9. 2013",
  note = "\url{http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData}"
}

@ONLINE{dbpedia_about,
  title = "DBpedia -- About"
  key = "DBpedia",
  author = "Christopher Sahnwaldt",
  cited = "4. 10. 2013",
  url = "http://dbpedia.org/About"
}

@article{linkeddata,
    abstract = {The term Linked Data refers to a set of best practices for publishing and connecting
structured data on the Web. These best practices have been adopted by an increasing
number of data providers over the last three years, leading to the creation of a global data
space containing billions of assertions - the Web of Data. In this article we present the
concept and technical principles of Linked Data, and situate these within the broader context
of related technological developments. We describe progress to date in publishing Linked
Data on the Web, review applications that have been developed to exploit the Web of Data,
and map out a research agenda for the Linked Data community as it moves forward.},
    author = {Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
    citeulike-article-id = {5008761},
    citeulike-linkout-0 = {http://dx.doi.org/10.4018/jswis.2009081901},
    doi = {10.4018/jswis.2009081901},
    editor = {Heath, T. and Hepp, M. and Bizer, C.},
    issn = {1552-6283},
    journal = {International Journal on Semantic Web and Information Systems (IJSWIS)},
    keywords = {linkeddata},
    month = {Mar},
    number = {3},
    pages = {1--22},
    posted-at = {2009-06-29 10:04:49},
    priority = {4},
    title = {Linked Data - The Story So Far},
    url = {http://dx.doi.org/10.4018/jswis.2009081901},
    volume = {5},
    year = {2009}
}

@InProceedings{stanford_ner,
  author    = {Finkel, Jenny Rose  and  Grenager, Trond  and  Manning, Christopher},
  title     = {Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05)},
  month     = {June},
  year      = {2005},
  address   = {Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  pages     = {363--370},
  url       = {http://www.aclweb.org/anthology/P/P05/P05-1045}
}

%@inproceedings{dbpedia,
%  --editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Uğur Doğan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
%  --publisher = {European Language Resources Association (ELRA)},
%  added-at = {2012-08-16T03:31:09.000+0200},
%  address = {Istanbul, Turkey},
%  author = {Mendes, Pablo N. and Jakob, Max and Bizer, Christian},
%  biburl = {http://www.bibsonomy.org/bibtex/24f12624140cbf1c31a561ae012b2520a/pablomendes},
%  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
%  interhash = {4e1af23de515a0387a3a6f1e89c88a1b},
%  intrahash = {4f12624140cbf1c31a561ae012b2520a},
%  isbn = {978-2-9517408-7-7},
%  keywords = {imported lod2page sys:relevantFor:lod2},
%  language = {english},
%  month = may,
%  timestamp = {2012-08-16T03:31:09.000+0200},
%  title = {DBpedia for NLP: A Multilingual Cross-domain Knowledge Base},
%  year = 2012
%}

@article{dbpedia,
    abstract = {{The DBpedia project is a community effort to extract structured information from Wikipedia and to make this information accessible on the Web. The resulting DBpedia knowledge base currently describes over 2.6 million entities. For each of these entities, DBpedia defines a globally unique identifier that can be dereferenced over the Web into a rich RDF description of the entity, including human-readable definitions in 30 languages, relationships to other resources, classifications in four concept hierarchies, various facts as well as data-level links to other Web data sources describing the entity. Over the last year, an increasing number of data publishers have begun to set data-level links to DBpedia resources, making DBpedia a central interlinking hub for the emerging Web of Data. Currently, the Web of interlinked data sources around DBpedia provides approximately 4.7 billion pieces of information and covers domains such as geographic information, people, companies, films, music, genes, drugs, books, and scientific publications. This article describes the extraction of the DBpedia knowledge base, the current status of interlinking DBpedia with other data sources on the Web, and gives an overview of applications that facilitate the Web of Data around DBpedia.}},
    address = {Amsterdam, The Netherlands},
    author = {Bizer, Christian and Lehmann, Jens and Kobilarov, Georgi and Auer, S\"{o}ren and Becker, Christian and Cyganiak, Richard and Hellmann, Sebastian},
    issn = {1570-8268},
    journal = {Journal of Web Semantics: Science, Services and Agents on the World Wide Web},
    keywords = {dbpedia},
    month = sep,
    pages = {154--165},
    priority = {2},
    publisher = {Elsevier Science Publishers B. V.},
    title = {{DBpedia -- A crystallization point for the Web of Data}},
    url = {http://portal.acm.org/citation.cfm?id=1640848},
    volume = {7},
    year = {2009}
}

% neuplna citace!
% @INPROCEEDINGS{stanford_ner,
%  author = {Jenny Rose Finkel and Trond Grenager and Christopher Manning},
%  editor = {Kevin Knight  and  Hwee Tou Ng  and  Kemal Oflazer},
%  title = {{Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling}},
%  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)},
%  year = 2005,
%  pages = {363--370},
%  url = {http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf}
%}

@inproceedings{illinois_ner,
 author = {Ratinov, Lev and Roth, Dan},
 title = {Design challenges and misconceptions in named entity recognition},
 booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning},
 series = {CoNLL '09},
 year = {2009},
 isbn = {978-1-932432-29-9},
% location = {Boulder, Colorado},
 pages = {147--155},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=1596374.1596399},
 acmid = {1596399},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 


%@inproceedings{illinois_ner,
%  author = {L. Ratinov and D. Roth},
%  title = {Design Challenges and Misconceptions in Named Entity Recognition},
%  booktitle = {CoNLL},
%  month = Jun,
%  year = 2009,
%  note = "\url{http://cogcomp.cs.illinois.edu/papers/RatinovRo09.pdf}",
%  funding = {MIAS, SoD, Library},
%  projects = {IE},
%  comment = {Named entity recognition; information extraction; knowledge resources; word class models; gazetteers; non-local features; global features; inference methods; BIO vs. BIOLU; text chunk representation},
%}

@book{nltk,
 author = {Bird, Steven and Klein, Ewan and Loper, Edward},
 title = {Natural Language Processing with Python},
 year = 2009,
 isbn = {0596516495, 9780596516499},
 edition = {1st},
 publisher = {O'Reilly Media, Inc.},
} 

@article{market_study_NER,
 author = {Marlies Olensky}, 
 title = {Market study on technical options for semantic feature extraction},
 year = 2012,
 month = Apr,
 note = "\url{http://ec.europa.eu/information_society/apps/projects/logos//2/270902/080/deliverables/001_DeliverableD74MarketStudyToolsSemExtracfinal2.pdf}",
 publisher = {Humboldt-Universität zu Berlin}
}
 
@article{blei_lda,
    abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.},
    address = {Cambridge, MA, USA},
    author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
    citeulike-article-id = {378143},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944919.944937},
    issn = {1532-4435},
    journal = {Journal of Machine Learning Research},
    keywords = {allocation, dirichlet, latent},
    month = mar,
    pages = {993--1022},
    posted-at = {2012-04-24 13:42:18},
    priority = {3},
    publisher = {JMLR.org},
    title = {Latent dirichlet allocation},
    url = {http://portal.acm.org/citation.cfm?id=944919.944937},
    volume = {3},
    year = {2003}
}

@inproceedings{gensim,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}
